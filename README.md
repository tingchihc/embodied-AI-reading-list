This repository offers a brief summary of essential papers and blogs on embodied AI, alongside a categorized collection of 3D scene representation, LLM agents papers and useful code repositories for starting your own project.

# Table of Contents  

Topic 1: <b>[Learning about 3D reconstruction and scene rendering](#nerf)</b>  
Topic 2: <b>[Learning about 3D scene representation](#3d-scene-rep)</b>  
Topic 3: <b>[Learning about LLM agents](#llm-agent)</b>  
Topic 4: <b>[Learning about Text-to-image/video](#t2iv)</b>  
Topic 5: <b>[Learning about Auto driving](#auto-drive)</b>  
Topic 6: <b>[Diffusion for Robotics and RL](#dif-RL)</b>  

<details>
  <summary><b>Topic 1: Learning about 3D reconstruction and scene rendering</b><a name="nerf"></a></summary>
  <ul>
    <li>(arxiv) Yuqi Zhang, et al. Efficient Large-scale Scene Representation with a Hybrid of High-resolution Grid and Plane Feature. <a href="https://arxiv.org/pdf/2303.03003">ğŸ“š</a> <a href="https://zyqz97.github.io/GP_NeRF/">ğŸŒ</a> </li>  
    <li>(ICLR'24) Francis Engelmann, et al. OpenNeRF: OpenSet 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views. <a href="https://arxiv.org/pdf/2404.03650">ğŸ“š</a> <a href="https://github.com/opennerf/opennerf">ğŸŒ</a> </li>  
  </ul>
</details>

<details>
  <summary><b>Topic 2: Learning about 3D scene representation</b><a name="3d-scene-rep"></a></summary>
  <ul>
    <li>(CVPR'24) Alexandros Delitzas, et al. SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes. <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Delitzas_SceneFun3D_Fine-Grained_Functionality_and_Affordance_Understanding_in_3D_Scenes_CVPR_2024_paper.pdf">ğŸ“š</a> <a href="https://scenefun3d.github.io/">ğŸŒ</a> </li>  
    <li>(CVPR'23) Songyou Peng, et al. OpenScene: 3D Scene Understanding with Open Vocabularies. <a href="https://arxiv.org/pdf/2211.15654">ğŸ“š</a> <a href="https://pengsongyou.github.io/openscene">ğŸŒ</a> </li>  
    <li>(NeurIPS'23) Yining Hong, et al. 3D-LLM: Injecting the 3D World into Large Language Models. <a href="https://arxiv.org/pdf/2307.12981">ğŸ“š</a> <a href="https://vis-www.cs.umass.edu/3dllm/">ğŸŒ</a> </li>  
    <li>(ICCV'23) Yicong Hong, et al. Learning Navigational Visual Representations with Semantic Map Supervision. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Learning_Navigational_Visual_Representations_with_Semantic_Map_Supervision_ICCV_2023_paper.pdf#:~:text=Inspired%20by%20the%20behavior%20that%20hu-mans%20naturally%20build">ğŸ“š</a> <a href="https://github.com/YicongHong/Ego2Map-NaViT">ğŸŒ</a> </li>  
    <li>(NeurIPS'23) AyÃ§a Takmaz, et al. OpenMask3D: Open-Vocabulary 3D Instance Segmentation. <a href="https://arxiv.org/pdf/2306.13631">ğŸ“š</a> <a href="https://openmask3d.github.io/">ğŸŒ</a> </li>  
    <li>(ICCV'23 <b>Oral</b>) Justin Kerr, et al. LERF: Language Embedded Radiance Fields. <a href="https://arxiv.org/pdf/2303.09553">ğŸ“š</a> <a href="https://www.lerf.io/">ğŸŒ</a> </li>  
  </ul>
</details>

<details>
  <summary><b>Topic 3: Learning about LLM agents</b><a name="llm-agent"></a></summary>
  <ul>
    <li>(COLM'24) Tianhua Tao, et al. CRYSTAL: Illuminating LLM Abilities on Language and Code. <a href="https://openreview.net/attachment?id=kWnlCVcp6o&name=pdf">ğŸ“š</a> <a href="https://www.llm360.ai/#crystal">ğŸŒ</a> </li>  
    <li>(COLM'24) Qingyun Wu, et al. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversations. <a href="https://openreview.net/attachment?id=BAakY1hNKS&name=pdf">ğŸ“š</a> <a href="https://github.com/microsoft/autogen">ğŸŒ</a> </li>  
    <li>(ECCV'24) Runsen Xu, et al. PointLLM: Empowering Large Language Models to Understand Point Clouds. <a href="https://arxiv.org/pdf/2308.16911">ğŸ“š</a> <a href="https://github.com/OpenRobotLab/PointLLM">ğŸŒ</a> </li>  
    <li>(ICML'24 <b>Oral</b>) Ziniu Hu, et al. SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code. <a href="https://openreview.net/attachment?id=gAyzjHw2ml&name=pdf">ğŸ“š</a> </li>  
  </ul>
</details>

<details>
  <summary><b>Topic 4: Learning about Text-to-image/video</b><a name="t2iv"></a></summary>
  <ul>
    <li>(COLM'24) Abhay Zala, et al. DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. <a href="https://openreview.net/attachment?id=NV8yRJRET1&name=pdf">ğŸ“š</a> <a href="https://diagrammergpt.github.io/">ğŸŒ</a> </li>  
    <li>(COLM'24) Han Lin, et al. VideoDirectorGPT: Consistent Multi-Scene Video Generation via LLM-Guided Planning. <a href="https://openreview.net/attachment?id=sKNIjS2brr&name=pdf">ğŸ“š</a> <a href="https://videodirectorgpt.github.io/">ğŸŒ</a> </li>  
  </ul>
</details>

<details>
  <summary><b>Topic 5: Learning about Auto driving</b><a name="auto-drive"></a></summary>
  <ul>
    <li>(COLM'24) Jiageng Mao, et al. A Language Agent for Autonomous Driving. <a href="https://openreview.net/attachment?id=UPE6WYE8vg&name=pdf">ğŸ“š</a> <a href="https://usc-gvl.github.io/Agent-Driver/">ğŸŒ</a> </li>  
    <li>(ICLR'24) Licheng Wen, et al. DiLuğŸ´: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models. <a href="https://arxiv.org/pdf/2309.16292">ğŸ“š</a> <a href="https://pjlab-adg.github.io/DiLu/">ğŸŒ</a> </li>  
  </ul>
</details>

<details>
  <summary><b>Diffusion for Robotics and RL</b><a name="dif-RL"></a></summary>
  <ul>
    <li>(SIGGRAPH Asia'24) Agon Serifi, et al. Robot Motion Diffusion Model: Motion Generation for Robotic Characters. <a href="https://la.disneyresearch.com/wp-content/uploads/RobotMDM_red.pdf">ğŸ“š</a> </li>  
  </ul>
</details>
